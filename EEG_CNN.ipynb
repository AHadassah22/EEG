{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/forrestbao/pyeeg.git\n",
        "import numpy as np\n",
        "import pyeeg as pe\n",
        "import pickle as pickle\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR8QME8whEGR",
        "outputId": "388813c7-fe55-44ea-b098-30e0d69b564e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/forrestbao/pyeeg.git\n",
            "  Cloning https://github.com/forrestbao/pyeeg.git to /tmp/pip-req-build-ba9bmk_d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/forrestbao/pyeeg.git /tmp/pip-req-build-ba9bmk_d\n",
            "  Resolved https://github.com/forrestbao/pyeeg.git to commit a6c18bb093e4748f9d9c208535a6ae024a0802b8\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from pyeeg==0.4.4) (1.22.4)\n",
            "Building wheels for collected packages: pyeeg\n",
            "  Building wheel for pyeeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyeeg: filename=pyeeg-0.4.4-py2.py3-none-any.whl size=28114 sha256=e101429834f3de53c38a8fcde0f0d2cc15e02f9fd48b468281a7fc31682cd933\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_7w9r88g/wheels/a8/c4/1a/cee09dcc12a11620066d35ace42e3c1e3bfbcc1db3a0ce7788\n",
            "Successfully built pyeeg\n",
            "Installing collected packages: pyeeg\n",
            "Successfully installed pyeeg-0.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical \n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import timeit\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D\n",
        "from keras.optimizers import SGD\n",
        "#import cv2, numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "KskFL9p6hNGE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsbCnuF9hRH3",
        "outputId": "61b45855-eba0-4c96-f010-6ac90ead7a1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_preprocessed_python\n",
        "os.getcwd()\n",
        "os.chdir('/content/drive/My Drive')\n",
        "     "
      ],
      "metadata": {
        "id": "9GgOlL0bhUzC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channel = [1,2,3,4,6,11,13,17,19,20,21,25,29,31] #14 Channels chosen to fit Emotiv Epoch+\n",
        "band = [4,8,12,16,25,45] #5 bands\n",
        "window_size = 256 #Averaging band power of 2 sec\n",
        "step_size = 16 #Each 0.125 sec update once\n",
        "sample_rate = 128 #Sampling rate of 128 Hz\n",
        "subjectList = ['01','02','03']\n",
        "#List of subjects"
      ],
      "metadata": {
        "id": "AoHpHet1hike"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FFT_Processing (sub, channel, band, window_size, step_size, sample_rate):\n",
        "    '''\n",
        "    arguments:  string subject\n",
        "                list channel indice\n",
        "                list band\n",
        "                int window size for FFT\n",
        "                int step size for FFT\n",
        "                int sample rate for FFT\n",
        "    return:     void\n",
        "    '''\n",
        "    meta = []\n",
        "    with open(\"/content/drive/My Drive/DEAPDATASET/data_preprocessed_python/s\" + sub + '.dat', 'rb') as file:\n",
        "\n",
        "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
        "\n",
        "        for i in range (0,40):\n",
        "            # loop over 0-39 trails\n",
        "            data = subject[\"data\"][i]\n",
        "            labels = subject[\"labels\"][i]\n",
        "            start = 0;\n",
        "\n",
        "            while start + window_size < data.shape[1]:\n",
        "                meta_array = []\n",
        "                meta_data = [] #meta vector for analysis\n",
        "                for j in channel:\n",
        "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
        "                    Y = pe.bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
        "                    meta_data = meta_data + list(Y[0])\n",
        "\n",
        "                meta_array.append(np.array(meta_data))\n",
        "                meta_array.append(labels)\n",
        "\n",
        "                meta.append(np.array(meta_array))    \n",
        "                start = start + step_size\n",
        "                \n",
        "        meta = np.array(meta)\n",
        "        np.save('/content/drive/My Drive/DEAPDATASET/data_preprocessed_python/s' + sub, meta, allow_pickle=True, fix_imports=True)"
      ],
      "metadata": {
        "id": "ZCVALKOrhmnK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for subjects in subjectList:\n",
        "    FFT_Processing (subjects, channel, band, window_size, step_size, sample_rate)"
      ],
      "metadata": {
        "id": "2uF4PWzIiCfq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_training = []\n",
        "label_training = []\n",
        "data_testing = []\n",
        "label_testing = []\n",
        "\n",
        "for subjects in subjectList:\n",
        "\n",
        "    with open('/content/drive/My Drive/DEAPDATASET/data_preprocessed_python/s' + subjects + '.npy', 'rb') as file:\n",
        "      sub = np.load(file,allow_pickle=True)\n",
        "      for i in range (0,sub.shape[0]):\n",
        "        if i % 5 == 0:\n",
        "          data_testing.append(sub[i][0])\n",
        "          label_testing.append(sub[i][1])\n",
        "        else:\n",
        "          data_training.append(sub[i][0])\n",
        "          label_training.append(sub[i][1])\n",
        "\n",
        "np.save('/content/drive/My Drive/DEAPDATASET/data_training', np.array(data_training), allow_pickle=True, fix_imports=True)\n",
        "np.save('/content/drive/My Drive/DEAPDATASET/label_training', np.array(label_training), allow_pickle=True, fix_imports=True)\n",
        "print(\"training dataset:\", np.array(data_training).shape, np.array(label_training).shape)\n",
        "\n",
        "np.save('/content/drive/My Drive/DEAPDATASET/data_testing', np.array(data_testing), allow_pickle=True, fix_imports=True)\n",
        "np.save('/content/drive/My Drive/DEAPDATASET/label_testing', np.array(label_testing), allow_pickle=True, fix_imports=True)\n",
        "print(\"testing dataset:\", np.array(data_testing).shape, np.array(label_testing).shape)\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQIJcBDSjF-F",
        "outputId": "0c6b8cd7-ce14-4f67-be33-719e01acffe2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training dataset: (46848, 70) (46848, 4)\n",
            "testing dataset: (11712, 70) (11712, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/DEAPDATASET/data_training.npy', 'rb') as fileTrain:\n",
        "    X  = np.load(fileTrain)\n",
        "    \n",
        "with open('/content/drive/My Drive/DEAPDATASET/label_training.npy', 'rb') as fileTrainL:\n",
        "    Y  = np.load(fileTrainL)\n",
        "    \n",
        "X = normalize(X)\n",
        "Z = np.ravel(Y[:, [3]])\n",
        "\n",
        "Arousal_Train = np.ravel(Y[:, [0]])\n",
        "Valence_Train = np.ravel(Y[:, [1]])\n",
        "Domain_Train = np.ravel(Y[:, [2]])\n",
        "Like_Train = np.ravel(Y[:, [3]])\n",
        "     "
      ],
      "metadata": {
        "id": "znCXCHq_jPMa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AFhzVvfkigo",
        "outputId": "0b1868c9-abb2-48dc-9340-de7948faef1b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46848, 70)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(Z)\n",
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkRJj9IFku_i",
        "outputId": "5b4ee23d-4318-4226-b801-b40fd94e3753"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XDpqsjOkxNR",
        "outputId": "4d599fe7-0858-46c7-f2f1-ff8c355c6018"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46848, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(X[:])"
      ],
      "metadata": {
        "id": "SRvLY2r5kzfQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('/content/drive/My Drive/DEAPDATASET/data_testing.npy', 'rb') as fileTrain:\n",
        "    M  = np.load(fileTrain)\n",
        "    \n",
        "with open('/content/drive/My Drive/DEAPDATASET/label_testing.npy', 'rb') as fileTrainL:\n",
        "    N  = np.load(fileTrainL)\n",
        "\n",
        "M = normalize(M)\n",
        "L = np.ravel(N[:, [3]])\n",
        "\n",
        "Arousal_Test = np.ravel(N[:, [0]])\n",
        "Valence_Test = np.ravel(N[:, [1]])\n",
        "Domain_Test = np.ravel(N[:, [2]])\n",
        "Like_Test = np.ravel(N[:, [3]])"
      ],
      "metadata": {
        "id": "S9X57FtHk22t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_test = np.array(M[:])"
      ],
      "metadata": {
        "id": "wc2HIIdyk7-h"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "y_test = to_categorical(L)\n",
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpo939dvk-L2",
        "outputId": "36cc8e2d-a97b-46f0-e6a5-e5cbd6fed7bb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_test[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18CiHb7SlAOy",
        "outputId": "40551de8-d5d7-4bf8-d7f9-39fa88332168"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.fit_transform(x_test)"
      ],
      "metadata": {
        "id": "q1JcDqwglB1J"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1], 1)"
      ],
      "metadata": {
        "id": "_Oxh98WwlDtz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80SoQmqmlFgk",
        "outputId": "ecb730f7-6f6b-4173-ad0a-8ac4f4388ee6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46848, 70, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "input_shape=(x_train.shape[1], 1)"
      ],
      "metadata": {
        "id": "3PKYbbqPlHGr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42I55ybUlKfo",
        "outputId": "10d6f3cb-125c-491f-f035-b8aaf3a89492"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
        "from keras.regularizers import l2"
      ],
      "metadata": {
        "id": "plpSInVulNnD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "intput_shape=(x_train.shape[1], 1)\n",
        "model.add(Conv1D(128, kernel_size=3,padding = 'same',activation='relu', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Conv1D(128,kernel_size=3,padding = 'same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Conv1D(64,kernel_size=3,padding = 'same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qs5qXbplNv5",
        "outputId": "37e01d54-4aa2-4e6e-aa99-31721efad282"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 70, 128)           512       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 70, 128)          512       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 35, 128)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 35, 128)           49280     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 35, 128)          512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 17, 128)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 17, 64)            24640     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 8, 64)            0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                32832     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                170       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 111,066\n",
            "Trainable params: 110,554\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "     "
      ],
      "metadata": {
        "id": "2XrgR7_3lRuV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,  \n",
        "          verbose=1,validation_data=(x_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_orzi3-NlU5O",
        "outputId": "10e85c81-92bc-4287-992b-0a0aaa362019"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "183/183 [==============================] - 40s 198ms/step - loss: 1.8562 - accuracy: 0.3246 - val_loss: 1.7390 - val_accuracy: 0.2951\n",
            "Epoch 2/200\n",
            "183/183 [==============================] - 39s 212ms/step - loss: 1.5150 - accuracy: 0.3896 - val_loss: 1.4153 - val_accuracy: 0.4316\n",
            "Epoch 3/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 1.3821 - accuracy: 0.4409 - val_loss: 1.2116 - val_accuracy: 0.5195\n",
            "Epoch 4/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 1.2850 - accuracy: 0.4805 - val_loss: 1.1371 - val_accuracy: 0.5371\n",
            "Epoch 5/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 1.2017 - accuracy: 0.5217 - val_loss: 1.0356 - val_accuracy: 0.5818\n",
            "Epoch 6/200\n",
            "183/183 [==============================] - 38s 208ms/step - loss: 1.1237 - accuracy: 0.5553 - val_loss: 0.9682 - val_accuracy: 0.6165\n",
            "Epoch 7/200\n",
            "183/183 [==============================] - 37s 200ms/step - loss: 1.0647 - accuracy: 0.5804 - val_loss: 0.9042 - val_accuracy: 0.6419\n",
            "Epoch 8/200\n",
            "183/183 [==============================] - 37s 204ms/step - loss: 1.0029 - accuracy: 0.6054 - val_loss: 0.8597 - val_accuracy: 0.6609\n",
            "Epoch 9/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.9457 - accuracy: 0.6288 - val_loss: 0.8501 - val_accuracy: 0.6591\n",
            "Epoch 10/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.9139 - accuracy: 0.6419 - val_loss: 0.7925 - val_accuracy: 0.6874\n",
            "Epoch 11/200\n",
            "183/183 [==============================] - 35s 192ms/step - loss: 0.8692 - accuracy: 0.6606 - val_loss: 0.7708 - val_accuracy: 0.6915\n",
            "Epoch 12/200\n",
            "183/183 [==============================] - 38s 208ms/step - loss: 0.8423 - accuracy: 0.6707 - val_loss: 0.7392 - val_accuracy: 0.7056\n",
            "Epoch 13/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.8078 - accuracy: 0.6862 - val_loss: 0.7164 - val_accuracy: 0.7213\n",
            "Epoch 14/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.7837 - accuracy: 0.6957 - val_loss: 0.7126 - val_accuracy: 0.7230\n",
            "Epoch 15/200\n",
            "183/183 [==============================] - 35s 191ms/step - loss: 0.7633 - accuracy: 0.6996 - val_loss: 0.6870 - val_accuracy: 0.7335\n",
            "Epoch 16/200\n",
            "183/183 [==============================] - 38s 208ms/step - loss: 0.7404 - accuracy: 0.7138 - val_loss: 0.6823 - val_accuracy: 0.7298\n",
            "Epoch 17/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.7237 - accuracy: 0.7186 - val_loss: 0.6829 - val_accuracy: 0.7337\n",
            "Epoch 18/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.7040 - accuracy: 0.7275 - val_loss: 0.6547 - val_accuracy: 0.7451\n",
            "Epoch 19/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.6929 - accuracy: 0.7326 - val_loss: 0.6456 - val_accuracy: 0.7458\n",
            "Epoch 20/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.6722 - accuracy: 0.7404 - val_loss: 0.6259 - val_accuracy: 0.7544\n",
            "Epoch 21/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.6670 - accuracy: 0.7418 - val_loss: 0.6275 - val_accuracy: 0.7533\n",
            "Epoch 22/200\n",
            "183/183 [==============================] - 38s 205ms/step - loss: 0.6423 - accuracy: 0.7524 - val_loss: 0.6050 - val_accuracy: 0.7651\n",
            "Epoch 23/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.6388 - accuracy: 0.7558 - val_loss: 0.6233 - val_accuracy: 0.7568\n",
            "Epoch 24/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.6143 - accuracy: 0.7637 - val_loss: 0.5985 - val_accuracy: 0.7717\n",
            "Epoch 25/200\n",
            "183/183 [==============================] - 36s 199ms/step - loss: 0.6142 - accuracy: 0.7631 - val_loss: 0.5966 - val_accuracy: 0.7724\n",
            "Epoch 26/200\n",
            "183/183 [==============================] - 38s 208ms/step - loss: 0.6066 - accuracy: 0.7659 - val_loss: 0.6021 - val_accuracy: 0.7723\n",
            "Epoch 27/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.5957 - accuracy: 0.7710 - val_loss: 0.5662 - val_accuracy: 0.7840\n",
            "Epoch 28/200\n",
            "183/183 [==============================] - 37s 201ms/step - loss: 0.5843 - accuracy: 0.7768 - val_loss: 0.5887 - val_accuracy: 0.7779\n",
            "Epoch 29/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.5763 - accuracy: 0.7794 - val_loss: 0.5868 - val_accuracy: 0.7818\n",
            "Epoch 30/200\n",
            "183/183 [==============================] - 36s 199ms/step - loss: 0.5700 - accuracy: 0.7833 - val_loss: 0.6028 - val_accuracy: 0.7790\n",
            "Epoch 31/200\n",
            "183/183 [==============================] - 38s 205ms/step - loss: 0.5569 - accuracy: 0.7880 - val_loss: 0.5824 - val_accuracy: 0.7839\n",
            "Epoch 32/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.5445 - accuracy: 0.7936 - val_loss: 0.5686 - val_accuracy: 0.7835\n",
            "Epoch 33/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.5418 - accuracy: 0.7972 - val_loss: 0.5522 - val_accuracy: 0.7975\n",
            "Epoch 34/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.5422 - accuracy: 0.7944 - val_loss: 0.5724 - val_accuracy: 0.7853\n",
            "Epoch 35/200\n",
            "183/183 [==============================] - 38s 209ms/step - loss: 0.5322 - accuracy: 0.7976 - val_loss: 0.5632 - val_accuracy: 0.7929\n",
            "Epoch 36/200\n",
            "183/183 [==============================] - 36s 194ms/step - loss: 0.5236 - accuracy: 0.8007 - val_loss: 0.5553 - val_accuracy: 0.7936\n",
            "Epoch 37/200\n",
            "183/183 [==============================] - 38s 208ms/step - loss: 0.5225 - accuracy: 0.8016 - val_loss: 0.5494 - val_accuracy: 0.7967\n",
            "Epoch 38/200\n",
            "183/183 [==============================] - 35s 192ms/step - loss: 0.5241 - accuracy: 0.8020 - val_loss: 0.5505 - val_accuracy: 0.7964\n",
            "Epoch 39/200\n",
            "183/183 [==============================] - 37s 205ms/step - loss: 0.5138 - accuracy: 0.8061 - val_loss: 0.5460 - val_accuracy: 0.8015\n",
            "Epoch 40/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.4931 - accuracy: 0.8148 - val_loss: 0.5584 - val_accuracy: 0.8026\n",
            "Epoch 41/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.4978 - accuracy: 0.8133 - val_loss: 0.5355 - val_accuracy: 0.8086\n",
            "Epoch 42/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.4986 - accuracy: 0.8114 - val_loss: 0.5385 - val_accuracy: 0.8056\n",
            "Epoch 43/200\n",
            "183/183 [==============================] - 38s 208ms/step - loss: 0.4813 - accuracy: 0.8185 - val_loss: 0.5334 - val_accuracy: 0.8075\n",
            "Epoch 44/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.4818 - accuracy: 0.8191 - val_loss: 0.5235 - val_accuracy: 0.8158\n",
            "Epoch 45/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.4723 - accuracy: 0.8228 - val_loss: 0.5188 - val_accuracy: 0.8128\n",
            "Epoch 46/200\n",
            "183/183 [==============================] - 35s 190ms/step - loss: 0.4684 - accuracy: 0.8238 - val_loss: 0.5154 - val_accuracy: 0.8111\n",
            "Epoch 47/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.4764 - accuracy: 0.8209 - val_loss: 0.5277 - val_accuracy: 0.8151\n",
            "Epoch 48/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.4722 - accuracy: 0.8233 - val_loss: 0.5326 - val_accuracy: 0.8120\n",
            "Epoch 49/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.4646 - accuracy: 0.8269 - val_loss: 0.5124 - val_accuracy: 0.8205\n",
            "Epoch 50/200\n",
            "183/183 [==============================] - 36s 195ms/step - loss: 0.4586 - accuracy: 0.8283 - val_loss: 0.5039 - val_accuracy: 0.8195\n",
            "Epoch 51/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.4577 - accuracy: 0.8287 - val_loss: 0.5229 - val_accuracy: 0.8144\n",
            "Epoch 52/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.4584 - accuracy: 0.8282 - val_loss: 0.5167 - val_accuracy: 0.8233\n",
            "Epoch 53/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.4493 - accuracy: 0.8330 - val_loss: 0.5313 - val_accuracy: 0.8107\n",
            "Epoch 54/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.4436 - accuracy: 0.8340 - val_loss: 0.5283 - val_accuracy: 0.8162\n",
            "Epoch 55/200\n",
            "183/183 [==============================] - 37s 202ms/step - loss: 0.4382 - accuracy: 0.8357 - val_loss: 0.5202 - val_accuracy: 0.8110\n",
            "Epoch 56/200\n",
            "183/183 [==============================] - 36s 194ms/step - loss: 0.4439 - accuracy: 0.8365 - val_loss: 0.5339 - val_accuracy: 0.8220\n",
            "Epoch 57/200\n",
            "183/183 [==============================] - 37s 201ms/step - loss: 0.4355 - accuracy: 0.8388 - val_loss: 0.5330 - val_accuracy: 0.8169\n",
            "Epoch 58/200\n",
            "183/183 [==============================] - 35s 194ms/step - loss: 0.4347 - accuracy: 0.8378 - val_loss: 0.5219 - val_accuracy: 0.8205\n",
            "Epoch 59/200\n",
            "183/183 [==============================] - 37s 202ms/step - loss: 0.4286 - accuracy: 0.8417 - val_loss: 0.5276 - val_accuracy: 0.8159\n",
            "Epoch 60/200\n",
            "183/183 [==============================] - 36s 195ms/step - loss: 0.4266 - accuracy: 0.8416 - val_loss: 0.5119 - val_accuracy: 0.8289\n",
            "Epoch 61/200\n",
            "183/183 [==============================] - 37s 201ms/step - loss: 0.4163 - accuracy: 0.8453 - val_loss: 0.4989 - val_accuracy: 0.8273\n",
            "Epoch 62/200\n",
            "183/183 [==============================] - 35s 192ms/step - loss: 0.4238 - accuracy: 0.8416 - val_loss: 0.5154 - val_accuracy: 0.8230\n",
            "Epoch 63/200\n",
            "183/183 [==============================] - 37s 204ms/step - loss: 0.4314 - accuracy: 0.8406 - val_loss: 0.4979 - val_accuracy: 0.8270\n",
            "Epoch 64/200\n",
            "183/183 [==============================] - 36s 195ms/step - loss: 0.4010 - accuracy: 0.8514 - val_loss: 0.5166 - val_accuracy: 0.8273\n",
            "Epoch 65/200\n",
            "183/183 [==============================] - 37s 202ms/step - loss: 0.4119 - accuracy: 0.8478 - val_loss: 0.4993 - val_accuracy: 0.8336\n",
            "Epoch 66/200\n",
            "183/183 [==============================] - 35s 190ms/step - loss: 0.4083 - accuracy: 0.8491 - val_loss: 0.5253 - val_accuracy: 0.8200\n",
            "Epoch 67/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.4115 - accuracy: 0.8490 - val_loss: 0.5327 - val_accuracy: 0.8252\n",
            "Epoch 68/200\n",
            "183/183 [==============================] - 36s 195ms/step - loss: 0.3923 - accuracy: 0.8549 - val_loss: 0.4966 - val_accuracy: 0.8363\n",
            "Epoch 69/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.4049 - accuracy: 0.8513 - val_loss: 0.4948 - val_accuracy: 0.8341\n",
            "Epoch 70/200\n",
            "183/183 [==============================] - 36s 195ms/step - loss: 0.3957 - accuracy: 0.8544 - val_loss: 0.5188 - val_accuracy: 0.8316\n",
            "Epoch 71/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.3892 - accuracy: 0.8569 - val_loss: 0.5046 - val_accuracy: 0.8314\n",
            "Epoch 72/200\n",
            "183/183 [==============================] - 36s 195ms/step - loss: 0.3803 - accuracy: 0.8591 - val_loss: 0.5286 - val_accuracy: 0.8288\n",
            "Epoch 73/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.3907 - accuracy: 0.8565 - val_loss: 0.4916 - val_accuracy: 0.8393\n",
            "Epoch 74/200\n",
            "183/183 [==============================] - 36s 195ms/step - loss: 0.3785 - accuracy: 0.8593 - val_loss: 0.5010 - val_accuracy: 0.8420\n",
            "Epoch 75/200\n",
            "183/183 [==============================] - 38s 205ms/step - loss: 0.3846 - accuracy: 0.8593 - val_loss: 0.4878 - val_accuracy: 0.8342\n",
            "Epoch 76/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.3836 - accuracy: 0.8597 - val_loss: 0.4825 - val_accuracy: 0.8359\n",
            "Epoch 77/200\n",
            "183/183 [==============================] - 38s 208ms/step - loss: 0.3792 - accuracy: 0.8611 - val_loss: 0.5077 - val_accuracy: 0.8355\n",
            "Epoch 78/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.3841 - accuracy: 0.8592 - val_loss: 0.5295 - val_accuracy: 0.8277\n",
            "Epoch 79/200\n",
            "183/183 [==============================] - 39s 211ms/step - loss: 0.3778 - accuracy: 0.8616 - val_loss: 0.4698 - val_accuracy: 0.8482\n",
            "Epoch 80/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.3669 - accuracy: 0.8648 - val_loss: 0.4883 - val_accuracy: 0.8388\n",
            "Epoch 81/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.3803 - accuracy: 0.8618 - val_loss: 0.4828 - val_accuracy: 0.8395\n",
            "Epoch 82/200\n",
            "183/183 [==============================] - 36s 195ms/step - loss: 0.3679 - accuracy: 0.8656 - val_loss: 0.5011 - val_accuracy: 0.8398\n",
            "Epoch 83/200\n",
            "183/183 [==============================] - 37s 202ms/step - loss: 0.3659 - accuracy: 0.8668 - val_loss: 0.4906 - val_accuracy: 0.8428\n",
            "Epoch 84/200\n",
            "183/183 [==============================] - 35s 191ms/step - loss: 0.3681 - accuracy: 0.8671 - val_loss: 0.4828 - val_accuracy: 0.8391\n",
            "Epoch 85/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.3580 - accuracy: 0.8705 - val_loss: 0.5279 - val_accuracy: 0.8324\n",
            "Epoch 86/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.3645 - accuracy: 0.8667 - val_loss: 0.4671 - val_accuracy: 0.8480\n",
            "Epoch 87/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.3523 - accuracy: 0.8725 - val_loss: 0.4707 - val_accuracy: 0.8467\n",
            "Epoch 88/200\n",
            "183/183 [==============================] - 35s 191ms/step - loss: 0.3542 - accuracy: 0.8698 - val_loss: 0.4886 - val_accuracy: 0.8440\n",
            "Epoch 89/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.3640 - accuracy: 0.8676 - val_loss: 0.5132 - val_accuracy: 0.8379\n",
            "Epoch 90/200\n",
            "183/183 [==============================] - 35s 191ms/step - loss: 0.3585 - accuracy: 0.8682 - val_loss: 0.4920 - val_accuracy: 0.8428\n",
            "Epoch 91/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.3523 - accuracy: 0.8718 - val_loss: 0.4781 - val_accuracy: 0.8475\n",
            "Epoch 92/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.3467 - accuracy: 0.8731 - val_loss: 0.4800 - val_accuracy: 0.8467\n",
            "Epoch 93/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.3404 - accuracy: 0.8757 - val_loss: 0.4779 - val_accuracy: 0.8491\n",
            "Epoch 94/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.3521 - accuracy: 0.8715 - val_loss: 0.4627 - val_accuracy: 0.8502\n",
            "Epoch 95/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.3518 - accuracy: 0.8710 - val_loss: 0.4904 - val_accuracy: 0.8500\n",
            "Epoch 96/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.3397 - accuracy: 0.8764 - val_loss: 0.4710 - val_accuracy: 0.8512\n",
            "Epoch 97/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.3387 - accuracy: 0.8768 - val_loss: 0.4849 - val_accuracy: 0.8490\n",
            "Epoch 98/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.3426 - accuracy: 0.8767 - val_loss: 0.4750 - val_accuracy: 0.8555\n",
            "Epoch 99/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.3369 - accuracy: 0.8782 - val_loss: 0.4902 - val_accuracy: 0.8498\n",
            "Epoch 100/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.3389 - accuracy: 0.8779 - val_loss: 0.4952 - val_accuracy: 0.8413\n",
            "Epoch 101/200\n",
            "183/183 [==============================] - 37s 202ms/step - loss: 0.3411 - accuracy: 0.8773 - val_loss: 0.4933 - val_accuracy: 0.8502\n",
            "Epoch 102/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.3409 - accuracy: 0.8770 - val_loss: 0.4870 - val_accuracy: 0.8469\n",
            "Epoch 103/200\n",
            "183/183 [==============================] - 37s 204ms/step - loss: 0.3347 - accuracy: 0.8774 - val_loss: 0.4941 - val_accuracy: 0.8509\n",
            "Epoch 104/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.3225 - accuracy: 0.8833 - val_loss: 0.4755 - val_accuracy: 0.8573\n",
            "Epoch 105/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.3387 - accuracy: 0.8787 - val_loss: 0.4889 - val_accuracy: 0.8473\n",
            "Epoch 106/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.3265 - accuracy: 0.8817 - val_loss: 0.4797 - val_accuracy: 0.8537\n",
            "Epoch 107/200\n",
            "183/183 [==============================] - 37s 203ms/step - loss: 0.3377 - accuracy: 0.8796 - val_loss: 0.4808 - val_accuracy: 0.8515\n",
            "Epoch 108/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.3274 - accuracy: 0.8825 - val_loss: 0.4580 - val_accuracy: 0.8598\n",
            "Epoch 109/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.3168 - accuracy: 0.8847 - val_loss: 0.4550 - val_accuracy: 0.8585\n",
            "Epoch 110/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.3160 - accuracy: 0.8863 - val_loss: 0.4682 - val_accuracy: 0.8596\n",
            "Epoch 111/200\n",
            "183/183 [==============================] - 37s 204ms/step - loss: 0.3180 - accuracy: 0.8859 - val_loss: 0.4852 - val_accuracy: 0.8571\n",
            "Epoch 112/200\n",
            "183/183 [==============================] - 36s 195ms/step - loss: 0.3234 - accuracy: 0.8850 - val_loss: 0.4788 - val_accuracy: 0.8529\n",
            "Epoch 113/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.3060 - accuracy: 0.8904 - val_loss: 0.4897 - val_accuracy: 0.8524\n",
            "Epoch 114/200\n",
            "183/183 [==============================] - 37s 201ms/step - loss: 0.3160 - accuracy: 0.8857 - val_loss: 0.4527 - val_accuracy: 0.8630\n",
            "Epoch 115/200\n",
            "183/183 [==============================] - 37s 204ms/step - loss: 0.3165 - accuracy: 0.8871 - val_loss: 0.4798 - val_accuracy: 0.8579\n",
            "Epoch 116/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.3131 - accuracy: 0.8878 - val_loss: 0.4765 - val_accuracy: 0.8570\n",
            "Epoch 117/200\n",
            "183/183 [==============================] - 37s 203ms/step - loss: 0.3091 - accuracy: 0.8890 - val_loss: 0.4600 - val_accuracy: 0.8587\n",
            "Epoch 118/200\n",
            "183/183 [==============================] - 37s 201ms/step - loss: 0.3175 - accuracy: 0.8858 - val_loss: 0.4803 - val_accuracy: 0.8528\n",
            "Epoch 119/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.3048 - accuracy: 0.8900 - val_loss: 0.4575 - val_accuracy: 0.8654\n",
            "Epoch 120/200\n",
            "183/183 [==============================] - 36s 199ms/step - loss: 0.3131 - accuracy: 0.8872 - val_loss: 0.5149 - val_accuracy: 0.8447\n",
            "Epoch 121/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.3099 - accuracy: 0.8888 - val_loss: 0.4647 - val_accuracy: 0.8613\n",
            "Epoch 122/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.3105 - accuracy: 0.8892 - val_loss: 0.4634 - val_accuracy: 0.8654\n",
            "Epoch 123/200\n",
            "183/183 [==============================] - 37s 203ms/step - loss: 0.2960 - accuracy: 0.8911 - val_loss: 0.4797 - val_accuracy: 0.8573\n",
            "Epoch 124/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.3014 - accuracy: 0.8942 - val_loss: 0.4836 - val_accuracy: 0.8473\n",
            "Epoch 125/200\n",
            "183/183 [==============================] - 37s 201ms/step - loss: 0.3077 - accuracy: 0.8881 - val_loss: 0.4877 - val_accuracy: 0.8513\n",
            "Epoch 126/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.2964 - accuracy: 0.8940 - val_loss: 0.4851 - val_accuracy: 0.8547\n",
            "Epoch 127/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.2978 - accuracy: 0.8949 - val_loss: 0.4754 - val_accuracy: 0.8624\n",
            "Epoch 128/200\n",
            "183/183 [==============================] - 36s 196ms/step - loss: 0.2970 - accuracy: 0.8937 - val_loss: 0.5008 - val_accuracy: 0.8510\n",
            "Epoch 129/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.3028 - accuracy: 0.8919 - val_loss: 0.5009 - val_accuracy: 0.8566\n",
            "Epoch 130/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.2976 - accuracy: 0.8942 - val_loss: 0.4724 - val_accuracy: 0.8582\n",
            "Epoch 131/200\n",
            "183/183 [==============================] - 38s 208ms/step - loss: 0.3013 - accuracy: 0.8926 - val_loss: 0.4523 - val_accuracy: 0.8689\n",
            "Epoch 132/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.2947 - accuracy: 0.8948 - val_loss: 0.4999 - val_accuracy: 0.8606\n",
            "Epoch 133/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.2869 - accuracy: 0.8975 - val_loss: 0.4885 - val_accuracy: 0.8621\n",
            "Epoch 134/200\n",
            "183/183 [==============================] - 40s 217ms/step - loss: 0.2922 - accuracy: 0.8957 - val_loss: 0.4599 - val_accuracy: 0.8671\n",
            "Epoch 135/200\n",
            "183/183 [==============================] - 35s 194ms/step - loss: 0.2898 - accuracy: 0.8976 - val_loss: 0.4679 - val_accuracy: 0.8659\n",
            "Epoch 136/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.2944 - accuracy: 0.8965 - val_loss: 0.4823 - val_accuracy: 0.8597\n",
            "Epoch 137/200\n",
            "183/183 [==============================] - 35s 193ms/step - loss: 0.2816 - accuracy: 0.8998 - val_loss: 0.4746 - val_accuracy: 0.8673\n",
            "Epoch 138/200\n",
            "183/183 [==============================] - 39s 215ms/step - loss: 0.2889 - accuracy: 0.8969 - val_loss: 0.4606 - val_accuracy: 0.8674\n",
            "Epoch 139/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.2871 - accuracy: 0.8984 - val_loss: 0.4633 - val_accuracy: 0.8657\n",
            "Epoch 140/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.2857 - accuracy: 0.8996 - val_loss: 0.4686 - val_accuracy: 0.8646\n",
            "Epoch 141/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.2854 - accuracy: 0.8992 - val_loss: 0.4811 - val_accuracy: 0.8647\n",
            "Epoch 142/200\n",
            "183/183 [==============================] - 37s 203ms/step - loss: 0.2887 - accuracy: 0.8969 - val_loss: 0.4555 - val_accuracy: 0.8695\n",
            "Epoch 143/200\n",
            "183/183 [==============================] - 37s 200ms/step - loss: 0.2783 - accuracy: 0.9022 - val_loss: 0.4607 - val_accuracy: 0.8692\n",
            "Epoch 144/200\n",
            "183/183 [==============================] - 38s 205ms/step - loss: 0.2808 - accuracy: 0.8998 - val_loss: 0.4871 - val_accuracy: 0.8607\n",
            "Epoch 145/200\n",
            "183/183 [==============================] - 37s 200ms/step - loss: 0.2710 - accuracy: 0.9038 - val_loss: 0.4588 - val_accuracy: 0.8706\n",
            "Epoch 146/200\n",
            "183/183 [==============================] - 38s 209ms/step - loss: 0.2771 - accuracy: 0.9009 - val_loss: 0.4719 - val_accuracy: 0.8652\n",
            "Epoch 147/200\n",
            "183/183 [==============================] - 40s 219ms/step - loss: 0.2751 - accuracy: 0.9042 - val_loss: 0.4710 - val_accuracy: 0.8640\n",
            "Epoch 148/200\n",
            "183/183 [==============================] - 36s 195ms/step - loss: 0.2780 - accuracy: 0.9031 - val_loss: 0.4787 - val_accuracy: 0.8606\n",
            "Epoch 149/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.2682 - accuracy: 0.9044 - val_loss: 0.4668 - val_accuracy: 0.8658\n",
            "Epoch 150/200\n",
            "183/183 [==============================] - 37s 200ms/step - loss: 0.2797 - accuracy: 0.9011 - val_loss: 0.5020 - val_accuracy: 0.8600\n",
            "Epoch 151/200\n",
            "183/183 [==============================] - 39s 212ms/step - loss: 0.2843 - accuracy: 0.8990 - val_loss: 0.4707 - val_accuracy: 0.8697\n",
            "Epoch 152/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.2718 - accuracy: 0.9037 - val_loss: 0.4814 - val_accuracy: 0.8625\n",
            "Epoch 153/200\n",
            "183/183 [==============================] - 39s 212ms/step - loss: 0.2778 - accuracy: 0.9012 - val_loss: 0.4720 - val_accuracy: 0.8688\n",
            "Epoch 154/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.2719 - accuracy: 0.9033 - val_loss: 0.4850 - val_accuracy: 0.8648\n",
            "Epoch 155/200\n",
            "183/183 [==============================] - 38s 205ms/step - loss: 0.2726 - accuracy: 0.9040 - val_loss: 0.4676 - val_accuracy: 0.8710\n",
            "Epoch 156/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.2641 - accuracy: 0.9085 - val_loss: 0.4600 - val_accuracy: 0.8700\n",
            "Epoch 157/200\n",
            "183/183 [==============================] - 35s 194ms/step - loss: 0.2784 - accuracy: 0.9010 - val_loss: 0.4720 - val_accuracy: 0.8700\n",
            "Epoch 158/200\n",
            "183/183 [==============================] - 40s 219ms/step - loss: 0.2697 - accuracy: 0.9048 - val_loss: 0.5040 - val_accuracy: 0.8620\n",
            "Epoch 159/200\n",
            "183/183 [==============================] - 37s 201ms/step - loss: 0.2714 - accuracy: 0.9039 - val_loss: 0.4531 - val_accuracy: 0.8751\n",
            "Epoch 160/200\n",
            "183/183 [==============================] - 39s 211ms/step - loss: 0.2638 - accuracy: 0.9072 - val_loss: 0.4864 - val_accuracy: 0.8628\n",
            "Epoch 161/200\n",
            "183/183 [==============================] - 37s 200ms/step - loss: 0.2626 - accuracy: 0.9074 - val_loss: 0.4720 - val_accuracy: 0.8724\n",
            "Epoch 162/200\n",
            "183/183 [==============================] - 38s 205ms/step - loss: 0.2668 - accuracy: 0.9055 - val_loss: 0.4803 - val_accuracy: 0.8706\n",
            "Epoch 163/200\n",
            "183/183 [==============================] - 37s 200ms/step - loss: 0.2600 - accuracy: 0.9082 - val_loss: 0.5085 - val_accuracy: 0.8642\n",
            "Epoch 164/200\n",
            "183/183 [==============================] - 37s 203ms/step - loss: 0.2714 - accuracy: 0.9049 - val_loss: 0.4652 - val_accuracy: 0.8673\n",
            "Epoch 165/200\n",
            "183/183 [==============================] - 40s 219ms/step - loss: 0.2683 - accuracy: 0.9062 - val_loss: 0.4728 - val_accuracy: 0.8730\n",
            "Epoch 166/200\n",
            "183/183 [==============================] - 36s 194ms/step - loss: 0.2620 - accuracy: 0.9077 - val_loss: 0.5025 - val_accuracy: 0.8611\n",
            "Epoch 167/200\n",
            "183/183 [==============================] - 39s 211ms/step - loss: 0.2661 - accuracy: 0.9068 - val_loss: 0.4753 - val_accuracy: 0.8664\n",
            "Epoch 168/200\n",
            "183/183 [==============================] - 37s 201ms/step - loss: 0.2551 - accuracy: 0.9098 - val_loss: 0.4742 - val_accuracy: 0.8702\n",
            "Epoch 169/200\n",
            "183/183 [==============================] - 38s 210ms/step - loss: 0.2648 - accuracy: 0.9074 - val_loss: 0.4602 - val_accuracy: 0.8750\n",
            "Epoch 170/200\n",
            "183/183 [==============================] - 37s 201ms/step - loss: 0.2520 - accuracy: 0.9112 - val_loss: 0.5035 - val_accuracy: 0.8626\n",
            "Epoch 171/200\n",
            "183/183 [==============================] - 38s 205ms/step - loss: 0.2572 - accuracy: 0.9095 - val_loss: 0.4907 - val_accuracy: 0.8708\n",
            "Epoch 172/200\n",
            "183/183 [==============================] - 39s 213ms/step - loss: 0.2564 - accuracy: 0.9097 - val_loss: 0.4690 - val_accuracy: 0.8700\n",
            "Epoch 173/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.2490 - accuracy: 0.9131 - val_loss: 0.4635 - val_accuracy: 0.8777\n",
            "Epoch 174/200\n",
            "183/183 [==============================] - 37s 205ms/step - loss: 0.2644 - accuracy: 0.9085 - val_loss: 0.4698 - val_accuracy: 0.8704\n",
            "Epoch 175/200\n",
            "183/183 [==============================] - 36s 200ms/step - loss: 0.2480 - accuracy: 0.9138 - val_loss: 0.4647 - val_accuracy: 0.8702\n",
            "Epoch 176/200\n",
            "183/183 [==============================] - 39s 212ms/step - loss: 0.2544 - accuracy: 0.9109 - val_loss: 0.4742 - val_accuracy: 0.8726\n",
            "Epoch 177/200\n",
            "183/183 [==============================] - 36s 199ms/step - loss: 0.2566 - accuracy: 0.9114 - val_loss: 0.4690 - val_accuracy: 0.8713\n",
            "Epoch 178/200\n",
            "183/183 [==============================] - 38s 205ms/step - loss: 0.2543 - accuracy: 0.9102 - val_loss: 0.4812 - val_accuracy: 0.8741\n",
            "Epoch 179/200\n",
            "183/183 [==============================] - 37s 200ms/step - loss: 0.2613 - accuracy: 0.9093 - val_loss: 0.5140 - val_accuracy: 0.8611\n",
            "Epoch 180/200\n",
            "183/183 [==============================] - 38s 206ms/step - loss: 0.2578 - accuracy: 0.9111 - val_loss: 0.4526 - val_accuracy: 0.8756\n",
            "Epoch 181/200\n",
            "183/183 [==============================] - 36s 199ms/step - loss: 0.2527 - accuracy: 0.9140 - val_loss: 0.4854 - val_accuracy: 0.8702\n",
            "Epoch 182/200\n",
            "183/183 [==============================] - 38s 205ms/step - loss: 0.2499 - accuracy: 0.9132 - val_loss: 0.4610 - val_accuracy: 0.8801\n",
            "Epoch 183/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.2448 - accuracy: 0.9154 - val_loss: 0.4935 - val_accuracy: 0.8706\n",
            "Epoch 184/200\n",
            "183/183 [==============================] - 37s 204ms/step - loss: 0.2555 - accuracy: 0.9108 - val_loss: 0.4727 - val_accuracy: 0.8741\n",
            "Epoch 185/200\n",
            "183/183 [==============================] - 39s 212ms/step - loss: 0.2466 - accuracy: 0.9160 - val_loss: 0.4850 - val_accuracy: 0.8731\n",
            "Epoch 186/200\n",
            "183/183 [==============================] - 36s 199ms/step - loss: 0.2503 - accuracy: 0.9131 - val_loss: 0.4768 - val_accuracy: 0.8752\n",
            "Epoch 187/200\n",
            "183/183 [==============================] - 39s 211ms/step - loss: 0.2391 - accuracy: 0.9172 - val_loss: 0.5161 - val_accuracy: 0.8650\n",
            "Epoch 188/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.2466 - accuracy: 0.9144 - val_loss: 0.5073 - val_accuracy: 0.8624\n",
            "Epoch 189/200\n",
            "183/183 [==============================] - 38s 209ms/step - loss: 0.2432 - accuracy: 0.9163 - val_loss: 0.4562 - val_accuracy: 0.8749\n",
            "Epoch 190/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.2339 - accuracy: 0.9200 - val_loss: 0.4704 - val_accuracy: 0.8752\n",
            "Epoch 191/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.2440 - accuracy: 0.9164 - val_loss: 0.4920 - val_accuracy: 0.8730\n",
            "Epoch 192/200\n",
            "183/183 [==============================] - 36s 198ms/step - loss: 0.2413 - accuracy: 0.9163 - val_loss: 0.4764 - val_accuracy: 0.8736\n",
            "Epoch 193/200\n",
            "183/183 [==============================] - 36s 197ms/step - loss: 0.2366 - accuracy: 0.9179 - val_loss: 0.4890 - val_accuracy: 0.8724\n",
            "Epoch 194/200\n",
            "183/183 [==============================] - 37s 201ms/step - loss: 0.2424 - accuracy: 0.9164 - val_loss: 0.4883 - val_accuracy: 0.8660\n",
            "Epoch 195/200\n",
            "183/183 [==============================] - 37s 204ms/step - loss: 0.2359 - accuracy: 0.9186 - val_loss: 0.4717 - val_accuracy: 0.8770\n",
            "Epoch 196/200\n",
            "183/183 [==============================] - 40s 216ms/step - loss: 0.2398 - accuracy: 0.9175 - val_loss: 0.4717 - val_accuracy: 0.8783\n",
            "Epoch 197/200\n",
            "183/183 [==============================] - 36s 199ms/step - loss: 0.2413 - accuracy: 0.9169 - val_loss: 0.4799 - val_accuracy: 0.8727\n",
            "Epoch 198/200\n",
            "183/183 [==============================] - 39s 211ms/step - loss: 0.2372 - accuracy: 0.9185 - val_loss: 0.4565 - val_accuracy: 0.8782\n",
            "Epoch 199/200\n",
            "183/183 [==============================] - 37s 201ms/step - loss: 0.2414 - accuracy: 0.9179 - val_loss: 0.5040 - val_accuracy: 0.8690\n",
            "Epoch 200/200\n",
            "183/183 [==============================] - 38s 207ms/step - loss: 0.2419 - accuracy: 0.9184 - val_loss: 0.5310 - val_accuracy: 0.8657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "6KcXYZR8lXDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33be38d7-511d-41d7-f5d0-f7768748bce9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "366/366 [==============================] - 3s 8ms/step - loss: 0.5310 - accuracy: 0.8657\n",
            "Test loss: 0.5310024619102478\n",
            "Test accuracy: 0.8656933307647705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert one-hot encoded labels to single labels\n",
        "y_test_single = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test_single, y_pred, average='macro')\n",
        "recall = recall_score(y_test_single, y_pred, average='macro')\n",
        "f1 = f1_score(y_test_single, y_pred, average='macro')\n",
        "\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1 Score:', f1)\n"
      ],
      "metadata": {
        "id": "cn4MnEFFl0wr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "440b323e-2ca1-49ac-e49a-aed6e4571fca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.86728940775453\n",
            "Recall: 0.862261942537889\n",
            "F1 Score: 0.8618642595064603\n"
          ]
        }
      ]
    }
  ]
}